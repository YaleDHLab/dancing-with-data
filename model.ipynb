{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed to control all randomness\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "\n",
    "set_random_seed(1)\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Inputs\n",
    "\n",
    "Let's load some local data and examine its contents. We'll play a small animation that contains the first few temporal frames of the input data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X.shape = n_body_parts, n_time_intervals, n_dimensions (3)\n",
    "X = np.load('data/npy/dance.npy')\n",
    "\n",
    "# labels[i] is a label for the ith body_part\n",
    "labels = ['AnnaC7.position', 'AnnaRFSH.position', 'AnnaRSHN.position', 'AnnaLIWR.position', 'AnnaRBWT.position', 'AnnaLOHAND.position', 'AnnaRFRM.position', 'AnnaT10.position', 'AnnaLMT5.position', 'AnnaLKNI.position', 'AnnaRIEL.position', 'AnnaRBHD.position', 'AnnaSolvingHips.position', 'AnnaLBHD.position', 'AnnaRIHAND.position', 'AnnaLBWT.position', 'AnnaRUPA.position', 'AnnaLFRM.position', 'AnnaRFHD.position', 'AnnaRIWR.position', 'AnnaLUPA.position', 'AnnaLFHD.position', 'AnnaRKNE.position', 'AnnaLFWT.position', 'AnnaLSHN.position', 'AnnaRTOE.position', 'AnnaLHEL.position', 'AnnaRKNI.position', 'AnnaCLAV.position', 'AnnaRHEL.position', 'AnnaMBWT.position', 'AnnaRMT5.position', 'AnnaARIEL.position', 'AnnaRBSH.position', 'AnnaLIHAND.position', 'AnnaLMT1.position', 'AnnaLTOE.position', 'AnnaRFWT.position', 'AnnaLabelingHips.position', 'AnnaMFWT.position', 'AnnaRANK.position', 'AnnaLOWR.position', 'AnnaLIEL.position', 'AnnaROHAND.position', 'AnnaRMT1.position', 'AnnaRTHI.position', 'AnnaLBSH.position', 'AnnaRELB.position', 'AnnaROWR.position', 'AnnaLANK.position', 'AnnaSTRN.position', 'AnnaLELB.position', 'AnnaLTHI.position', 'AnnaLFSH.position', 'AnnaLKNE.position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from mpl_toolkits.mplot3d.art3d import juggle_axes\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "import matplotlib\n",
    "\n",
    "# ask matplotlib to plot up to 2^128 frames in animations\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "\n",
    "def update_points(time, points, X):\n",
    "  points._offsets3d = juggle_axes(X[:,time,0], X[:,time,1], X[:,time,2], 'z')\n",
    "\n",
    "def get_plot(X, lim=2, frames=200, duration=45):\n",
    "  fig = plt.figure()\n",
    "  ax = p3.Axes3D(fig)\n",
    "  ax.set_xlim(-lim, lim)\n",
    "  ax.set_ylim(-lim, lim)\n",
    "  ax.set_zlim(-lim, lim)\n",
    "  points = ax.scatter(X[:,0,0], X[:,0,1], X[:,0,2], depthshade=False) # x,y,z vals\n",
    "  return animation.FuncAnimation(fig,\n",
    "    update_points,\n",
    "    frames,\n",
    "    interval=duration,\n",
    "    fargs=(points, X),\n",
    "    blit=False  \n",
    "  ).to_jshtml()\n",
    "\n",
    "if False:\n",
    "  HTML(get_plot(X, frames=int(X.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center each of the 3 dimensional features\n",
    "X -= np.amin(X, axis=(0, 1))\n",
    "X /= np.amax(X, axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# via https://github.com/cpmpercussion/keras-mdn-layer\n",
    "\n",
    "'''\n",
    "A Mixture Density Layer for Keras\n",
    "cpmpercussion: Charles Martin (University of Oslo) 2018\n",
    "https://github.com/cpmpercussion/keras-mdn-layer\n",
    "\n",
    "Hat tip to [Omimo's Keras MDN layer](https://github.com/omimo/Keras-MDN) for a starting point for this code.\n",
    "'''\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "def elu_activation(x, plus_one=False):\n",
    "  '''Exponential Linear Unit activation with a very small addition to help prevent NaN in loss.'''\n",
    "  if plus_one:\n",
    "    return (K.elu(x) + 1 + 1e-8)\n",
    "  return (K.elu(x) + 1e-8)\n",
    "\n",
    "\n",
    "class MDN(Layer):\n",
    "  '''A Mixture Density Network Layer for Keras.\n",
    "  This layer has a few tricks to avoid NaNs in the loss function when training:\n",
    "    - Activation for variances is ELU + 1 + 1e-8 (to avoid very small values)\n",
    "    - Mixture weights (pi) are trained in as logits, not in the softmax space.\n",
    "\n",
    "  A loss function needs to be constructed with the same output dimension and number of mixtures.\n",
    "  A sampling function is also provided to sample from distribution parametrised by the MDN outputs.\n",
    "  '''\n",
    "\n",
    "  def __init__(self, output_dimension, num_mixtures, **kwargs):\n",
    "    self.output_dim = output_dimension\n",
    "    self.num_mix = num_mixtures\n",
    "    with tf.name_scope('MDN'):\n",
    "      self.mdn_mus = Dense(self.num_mix * self.output_dim, name='mdn_mus') # mix*output vals, no activation\n",
    "      self.mdn_sigmas = Dense(self.num_mix * self.output_dim, activation=elu_activation, name='mdn_sigmas') # mix*output vals exp activation\n",
    "      self.mdn_pi = Dense(self.num_mix, name='mdn_pi') # mix vals, logits\n",
    "    super(MDN, self).__init__(**kwargs)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.mdn_mus.build(input_shape)\n",
    "    self.mdn_sigmas.build(input_shape)\n",
    "    self.mdn_pi.build(input_shape)\n",
    "    self.trainable_weights = self.mdn_mus.trainable_weights + self.mdn_sigmas.trainable_weights + self.mdn_pi.trainable_weights\n",
    "    self.non_trainable_weights = self.mdn_mus.non_trainable_weights + self.mdn_sigmas.non_trainable_weights + self.mdn_pi.non_trainable_weights\n",
    "    super(MDN, self).build(input_shape)\n",
    "\n",
    "  def call(self, x, mask=None):\n",
    "    with tf.name_scope('MDN'):\n",
    "      mdn_out = keras.layers.concatenate([\n",
    "        self.mdn_mus(x),\n",
    "        self.mdn_sigmas(x),\n",
    "        self.mdn_pi(x)\n",
    "      ], name='mdn_outputs')\n",
    "    return mdn_out\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    '''Returns output shape, showing the number of mixture parameters.'''\n",
    "    return (input_shape[0], (2 * self.output_dim * self.num_mix) + self.num_mix)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "      'output_dimension': self.output_dim,\n",
    "      'num_mixtures': self.num_mix\n",
    "    }\n",
    "    base_config = super(MDN, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def get_mixture_loss_func(output_dim, num_mixes):\n",
    "  '''Construct a loss functions for the MDN layer parametrised by number of mixtures.'''\n",
    "  # Construct a loss function with the right number of mixtures and outputs\n",
    "  def loss_func(y_true, y_pred):\n",
    "    # Reshape inputs in case this is used in a TimeDistribued layer\n",
    "    y_pred = tf.reshape(y_pred, [-1, (2 * num_mixes * output_dim) + num_mixes], name='reshape_ypreds')\n",
    "    y_true = tf.reshape(y_true, [-1, output_dim], name='reshape_ytrue')\n",
    "    # Split the inputs into paramaters\n",
    "    out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[\n",
    "      num_mixes * output_dim,\n",
    "      num_mixes * output_dim,\n",
    "      num_mixes\n",
    "    ], axis=-1, name='mdn_coef_split')\n",
    "    # produces flat list that contains `num_mixes` instances of `output_dim` [n, n, n, ...]\n",
    "    component_splits = [output_dim] * num_mixes\n",
    "    # produces `num_mixes` arrays with the mus\n",
    "    mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "    # produces `num_mixes` arrays with the sigs\n",
    "    sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "    cat = tfd.Categorical(logits=out_pi)\n",
    "    # produces num_mixes arrays each with a multivariate normal distribution with a single mu and sigma\n",
    "    coll = [tfd.MultivariateNormalDiag(loc=mu, scale_diag=sig) for mu, sig in zip(mus, sigs)]\n",
    "    mixture = tfd.Mixture(cat=cat, components=coll)\n",
    "    loss = mixture.log_prob(y_true)\n",
    "    loss = tf.negative(loss)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "  # Actually return the loss_func\n",
    "  with tf.name_scope('MDN'):\n",
    "    return loss_func\n",
    "\n",
    "\n",
    "def split_mixture_params(params, output_dim, num_mixes):\n",
    "  '''Splits up an array of mixture parameters into mus, sigmas, and pis\n",
    "  depending on the number of mixtures and output dimension.'''\n",
    "  mus = params[:num_mixes*output_dim]\n",
    "  sigs = params[num_mixes*output_dim:2*num_mixes*output_dim]\n",
    "  pi_logits = params[-num_mixes:]\n",
    "  return mus, sigs, pi_logits\n",
    "\n",
    "\n",
    "def softmax(w, t=1.0):\n",
    "  '''Softmax function for a list or numpy array of logits. Also adjusts temperature.'''\n",
    "  e = np.array(w) / t  # adjust temperature\n",
    "  e -= e.max()  # subtract max to protect from exploding exp values.\n",
    "  e = np.exp(e)\n",
    "  dist = e / np.sum(e)\n",
    "  return dist\n",
    "\n",
    "\n",
    "def sample_from_output(params, output_dim, num_mixes, temp=1.0):\n",
    "  '''Sample from an MDN output with temperature adjustment.'''\n",
    "  mus = params[:num_mixes*output_dim]\n",
    "  sigs = params[num_mixes*output_dim:2*num_mixes*output_dim]\n",
    "  pis = softmax(params[-num_mixes:], t=temp)\n",
    "  m = sample_from_categorical(pis)\n",
    "  # Alternative way to sample from categorical:\n",
    "  # m = np.random.choice(range(len(pis)), p=pis)\n",
    "  mus_vector = mus[m*output_dim:(m+1)*output_dim]\n",
    "  sig_vector = sigs[m*output_dim:(m+1)*output_dim] * temp  # adjust for temperature\n",
    "  cov_matrix = np.identity(output_dim) * sig_vector\n",
    "  sample = np.random.multivariate_normal(mus_vector, cov_matrix, 1)\n",
    "  return sample\n",
    "\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "  '''Samples from a categorical model PDF.'''\n",
    "  r = np.random.rand(1)  # uniform random number in [0,1]\n",
    "  accumulate = 0\n",
    "  for i in range(0, dist.size):\n",
    "    accumulate += dist[i]\n",
    "    if accumulate >= r:\n",
    "      return i\n",
    "  tf.logging.info('Error sampling mixture model.')\n",
    "  return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation, CuDNNLSTM\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import keras, os\n",
    "\n",
    "# config\n",
    "look_back = 32 # number of previous time slices to use to predict the time positions at time `i`\n",
    "lstm_cells = 64 # number of cells in each lstm layer\n",
    "n_features = int(X.shape[0]) * int(X.shape[2]) # number of coordinate values to be predicted by each of `m` models\n",
    "input_shape = (look_back, n_features) # shape of each input feature\n",
    "use_mdn = True # whether to use the MDN final layer or not\n",
    "m = 2 # number of gaussian models to build if use_mdn == True\n",
    "\n",
    "# use tensorflow backend\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "# determine the LSTM cells to use (hinges on whether GPU is available to keras)\n",
    "gpus = K.tensorflow_backend._get_available_gpus()\n",
    "LSTM_UNIT = CuDNNLSTM if len(gpus) > 0 else LSTM\n",
    "print('GPUs found:', gpus)\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(LSTM_UNIT(lstm_cells, return_sequences=True, input_shape=input_shape, ))\n",
    "model.add(LSTM_UNIT(lstm_cells, return_sequences=True, batch_input_shape=(s), ))\n",
    "model.add(LSTM_UNIT(lstm_cells, batch_input_shape=(s),))\n",
    "model.add(Dense(lstm_cells))\n",
    "\n",
    "if use_mdn:\n",
    "  model.add(MDN(n_features, m))\n",
    "  model.compile(loss=get_mixture_loss_func(n_features, m), optimizer=Adam(lr=0.0005))\n",
    "else:\n",
    "  model.add(Dense(n_features, activation='tanh'))\n",
    "  model.compile(loss=mean_squared_error, optimizer='sgd')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x has shape: n_samples, look_back, n_vertices*3\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "n_obs, n_time, n_attrs = [int(i) for i in X.shape]\n",
    "\n",
    "for i in range(look_back, n_time-1, 1):\n",
    "  train_x.append( X[:, i-look_back:i, :].reshape(look_back, n_obs * n_attrs) )\n",
    "  train_y.append( X[:, i            , :].reshape(n_obs * n_attrs) )\n",
    "  \n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check untrained (baseline) accuracy\n",
    "samples = 1\n",
    "model.evaluate(train_x[:samples], train_y[:samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "# fit the model\n",
    "model.fit(train_x, train_y, epochs=20, batch_size=1, callbacks=[PlotLossesKeras()], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/dance.model'\n",
    "weights_path = 'models/dance.weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('models'): os.makedirs('models')\n",
    "\n",
    "model.save(model_path)\n",
    "model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_mdn:\n",
    "  model = keras.models.load_model(model_path, custom_objects={\n",
    "    'MDN': MDN,\n",
    "    'loss_func': get_mixture_loss_func(n_features, m),\n",
    "  })\n",
    "else:\n",
    "  model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate `n_frames` of new output time slices\n",
    "n_frames = 1000\n",
    "\n",
    "# data will copy the first few frames of X then add new frame predictions\n",
    "data = X[:, 0:look_back, :]\n",
    "\n",
    "t0, t1, t2 = [int(i) for i in train_x.shape]\n",
    "d0, d1, d2 = [int(i) for i in data.shape]\n",
    "\n",
    "for i in range(look_back, n_frames, 1):\n",
    "  # get the model's prediction for the position of points at time `i`. result.shape = (1, d0 * d2)\n",
    "  # here we're only feeding one observation from train_x; the indexing +1 just reshapes the observation\n",
    "  # from a, b into 1, a, b\n",
    "  result = model.predict(train_x[i:i+1])\n",
    "  # if using the mixed density network, pull out vals that describe vertex positions\n",
    "  if use_mdn:\n",
    "    result = np.apply_along_axis(sample_from_output, 1, result, n_features, m, temp=1.0)\n",
    "  # reshape the result into the form of a single time slice in `X` (or `data`)\n",
    "  result = result.reshape(d0, 1, d2)\n",
    "  # use the result to generate a new training data slice that includes `look_back` observations\n",
    "  stacked = np.concatenate( (data[:, i-look_back+1:i, ], result),  axis=1)\n",
    "  # transform the shape of the stacked observation into a single new training observation's shape\n",
    "  stacked = stacked.reshape(1, look_back, d0 * d2)\n",
    "  # add the new training observation to the array of training observations\n",
    "  train_x = np.vstack((train_x, stacked))  \n",
    "  # add the result's new time slice to the stack of time slices in `data`\n",
    "  data = np.concatenate((data, result), axis=1)\n",
    "  \n",
    "# remove the first look_back time frames as they were used to seed subsequent observations\n",
    "data = data[:, look_back:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip the first look_back frames\n",
    "HTML(get_plot(data, frames=n_frames - look_back - 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
