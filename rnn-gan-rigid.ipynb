{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#input_file = 'mariel-1.npy'\n",
    "#input_file = 'carrie-10-min-scaled.npy'\n",
    "input_file = 'carrie-10-mins.npy'\n",
    "\n",
    "X0 = np.load(os.path.join(\"data/npy\", input_file))\n",
    "#X0 = np.load(os.path.join(input_file))\n",
    "\n",
    "if input_file == 'mariel-1.npy':\n",
    "    X0 = X0.transpose((1,0,2))\n",
    "    X0 = X0[1300:7000]\n",
    "    X0[:,:,2] *= -1\n",
    "    X = X0.copy()\n",
    "    X -= X.mean(axis=(0,1))\n",
    "    X *= 0.5\n",
    "elif input_file == 'carrie-10-min-scaled.npy':\n",
    "    X0 = X0.transpose((1,0,2))\n",
    "    X = X0.copy()\n",
    "    X[:,:,1] = X0[:,:,2]\n",
    "    X[:,:,2] = X0[:,:,1]\n",
    "    X = X[100:]\n",
    "    X[:,:,:2] -= 0.25\n",
    "if input_file == 'carrie-10-mins.npy':\n",
    "    X0 = X0.transpose((1,0,2))\n",
    "    X0 = X0[100:]\n",
    "    X0[:,:,2] *= -1\n",
    "    X = X0.copy()\n",
    "    X -= X.mean(axis=(0,1))\n",
    "    X *= 0.5\n",
    "    \n",
    "\n",
    "# set to True if you want to cluster the 55 vertices into a\n",
    "# reduced set of 8 vertices\n",
    "do_cluster = False\n",
    "if do_cluster:\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    nc = 8\n",
    "    km = KMeans(n_clusters=nc).fit(X.transpose((1,2,0)).reshape(X.shape[1],-1))\n",
    "    Xcl = np.zeros((X.shape[0], nc, 3))\n",
    "    for i in range(nc):\n",
    "        Xcl[:,i] = X[:,np.argwhere(km.labels_==i)[:,0]].mean(axis=1)\n",
    "    X = Xcl\n",
    "    \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.mean(axis=1));\n",
    "plt.figure()\n",
    "plt.hist([X[:,:,0].flatten(),X[:,:,1].flatten(),X[:,:,2].flatten()], histtype='step');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.max(axis=(0,1)))\n",
    "print(X.min(axis=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.mean(axis=(0,1)))\n",
    "print(X.std(axis=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from mpl_toolkits.mplot3d.art3d import juggle_axes\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# ask matplotlib to plot up to 2^128 frames in animations\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "\n",
    "def update_points(time, points, df):\n",
    "  '''\n",
    "  Callback function called by plotting function below. Mutates the vertex\n",
    "  positions of each value in `points` so the animation moves\n",
    "  @param int time: the index of the time slice to visualize within `df`\n",
    "  @param mpl_toolkits.mplot3d.art3d.Path3DCollection points: the actual\n",
    "    geometry collection whose internal values this function mutates to move\n",
    "    the displayed points\n",
    "  @param numpy.ndarray df: a numpy array with the following three axes:\n",
    "    df.shape[0] = n_vertices\n",
    "    df.shape[1] = n_time_slices\n",
    "    df.shape[2] = n_dimensions\n",
    "  '''\n",
    "  points._offsets3d = juggle_axes(df[time,:,0], df[time,:,1], df[time,:,2], 'z')\n",
    "\n",
    "def animate(seq, frames=None, axis_min=-0.75, axis_max=0.75, speed=45):\n",
    "    if frames is None:\n",
    "        frames = len(seq)\n",
    "    fig = plt.figure()\n",
    "    ax = p3.Axes3D(fig)\n",
    "    ax.set_xlim(axis_min, axis_max)\n",
    "    ax.set_ylim(axis_min, axis_max)\n",
    "    ax.set_zlim(axis_min, axis_max)\n",
    "    points = ax.scatter(seq[0,:,0], seq[0,:,1], seq[0,:,2], depthshade=False)\n",
    "    return animation.FuncAnimation(\n",
    "        fig,\n",
    "        update_points,\n",
    "        frames,\n",
    "        interval=speed,\n",
    "        fargs=(points, seq),\n",
    "        blit=False,\n",
    "    ).to_jshtml()\n",
    "\n",
    "def plot_pose(x, ax=None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = p3.Axes3D(fig)\n",
    "    ax.set_xlim(-0.75,0.75)\n",
    "    ax.set_ylim(-0.75,0.75)\n",
    "    ax.set_zlim(-0.75,0.75)\n",
    "    ax.scatter(x[:,0], x[:,1], x[:,2])\n",
    "    return ax\n",
    "\n",
    "def plot_stick(x, edges, ax=None, figsize=None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = p3.Axes3D(fig)\n",
    "    ax.set_xlim(-0.75,0.75)\n",
    "    ax.set_ylim(-0.75,0.75)\n",
    "    ax.set_zlim(-0.75,0.75)\n",
    "    #ax.scatter(x[:,0], x[:,1], x[:,2])\n",
    "    for e in edges:\n",
    "        ax.plot(np.linspace(x[e[0],0],x[e[1],0],10),np.linspace(x[e[0],1],x[e[1],1],10),np.linspace(x[e[0],2],x[e[1],2],10))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdist_mean = np.zeros((X.shape[1],X.shape[1]))\n",
    "vdist_var = np.zeros_like(vdist_mean)\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(i+1,X.shape[1]):\n",
    "        vdist = np.sum((X[:,i]-X[:,j])**2, axis=-1)\n",
    "        vdist_mean[i,j] = vdist_mean[j,i] = vdist.mean()\n",
    "        vdist_var[i,j] = vdist_var[j,i] = vdist.var(ddof=1)\n",
    "vdist_std = np.sqrt(vdist_var)\n",
    "\n",
    "vdist_mean_q5, vdist_mean_q95 = np.quantile(vdist_mean[vdist_mean>0], (0.05,0.95))\n",
    "vdist_var_q5, vdist_var_q95 = np.quantile(vdist_var[vdist_var>0], (0.05,0.95))\n",
    "vdist_std_q5, vdist_std_q95 = np.quantile(vdist_std[vdist_std>0], (0.05,0.95))\n",
    "print(\"pairwise mean quantiles:  5th=%.2e  95th=%.2e\" % (vdist_mean_q5, vdist_mean_q95))\n",
    "print(\"pairwise std. quantiles:  5th=%.2e  95th=%.2e\" % (vdist_std_q5, vdist_std_q95))\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(np.log10(vdist_mean[vdist_mean>0]), bins=40);\n",
    "plt.axvline(np.log10(vdist_mean_q5), color='red', lw=1);\n",
    "plt.axvline(np.log10(vdist_mean_q95), color='red', lw=1);\n",
    "plt.xlabel(\"log10(pairwise mean)\", fontsize=14)\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(np.log10(vdist_std[vdist_std>0]), bins=40);\n",
    "plt.xlabel(\"log10(pairwise std.)\", fontsize=14);\n",
    "plt.axvline(np.log10(vdist_std_q5), color='red', lw=1);\n",
    "plt.axvline(np.log10(vdist_std_q95), color='red', lw=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4.5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(vdist_mean, vmin=vdist_mean_q5, vmax=vdist_mean_q95, norm=LogNorm())\n",
    "plt.colorbar()\n",
    "plt.title(\"pairwise mean\", fontsize=14)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(vdist_std, vmin=vdist_std_q5, vmax=vdist_mean_q95, norm=LogNorm())\n",
    "plt.title(\"pairwise std.\", fontsize=14);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_triangle = np.triu_indices_from(vdist_var, k=1)\n",
    "variances = vdist_var[upper_triangle]\n",
    "vtx_pairs = sorted(zip(*upper_triangle), key=lambda p: vdist_var[p[0],p[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rigid = 140\n",
    "#n_rigid = 7\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(sorted(vdist_var[upper_triangle]))\n",
    "plt.yscale('log')\n",
    "plt.ylabel('pairwise variance', fontsize=14)\n",
    "plt.xlabel('pair #', fontsize=14)\n",
    "plt.axvline(n_rigid, color='red', lw=1);\n",
    "frame_num = 500\n",
    "ax = plot_stick(X[frame_num], vtx_pairs[:n_rigid], figsize=(8,8))\n",
    "plot_pose(X[frame_num], ax);\n",
    "plt.title(\"Reference pose (frame %d)\"%frame_num, fontsize=16)\n",
    "ax.get_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# config for Gatsby cores\n",
    "if 'gatsby' in os.environ['HOSTNAME']:\n",
    "\n",
    "    # specify target gpu device\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0' # either '0' or '1' to utilize Titan X GPUs\n",
    "\n",
    "    # allow dynamic GPU allocation\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    \n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_model(lookback=60, npred=4, n_cells=(48,48,48), n_cells_disc=(48,48,48,), noise_dim=128, center_seq=False, n_rigid=0, use_mse=False):\n",
    "    K.clear_session()\n",
    "    probe = None\n",
    "    \n",
    "    input_seq = layers.Input((lookback, X.shape[1], X.shape[2]))\n",
    "    H = input_seq\n",
    "    \n",
    "    input_noise = layers.Input((noise_dim,))\n",
    "    Hn = input_noise\n",
    "    \n",
    "    \n",
    "    # first, translate each input sequences in the batch so it's centered at 0,0\n",
    "    if center_seq:\n",
    "        # mask [1,1,0] to ignore z-axis shift\n",
    "        offsets = layers.Lambda(lambda x: K.constant([[[[1,1,0]]]])*K.mean(x,axis=(1,2),keepdims=True))(H)\n",
    "        H = layers.Subtract()([H,offsets])\n",
    "        probe = Model(input_seq, H)\n",
    "        \n",
    "    H = layers.Reshape((lookback, X.shape[1]*X.shape[2]))(H)\n",
    "    \n",
    "    for nc in n_cells:\n",
    "        H = layers.CuDNNLSTM(nc, return_sequences=True)(H)\n",
    "    #H = layers.CuDNNLSTM(n_cells[-1])(H)\n",
    "    H = layers.Cropping1D((lookback-npred,0))(H)\n",
    "    \n",
    "    Hn = layers.Dense(npred*n_cells[-1])(Hn)\n",
    "    Hn = layers.LeakyReLU()(Hn)\n",
    "    Hn = layers.Reshape((npred,n_cells[-1]))(Hn)\n",
    "    \n",
    "    H = layers.Add()([H,Hn])\n",
    "    \n",
    "    H = layers.Dense(X.shape[1]*X.shape[2])(H)\n",
    "    \n",
    "    H = layers.Reshape((npred,X.shape[1],X.shape[2]))(H)\n",
    "    \n",
    "    # if we shifted before processing, shift back before the output\n",
    "    if center_seq:\n",
    "        H = layers.Add()([H, offsets])\n",
    "    \n",
    "    output_seq = H\n",
    "    \n",
    "    generator = Model([input_seq, input_noise], output_seq)\n",
    "    \n",
    "    disc_input1 = H1 = layers.Input((lookback, X.shape[1], X.shape[2]))\n",
    "    disc_input2 = H2 = layers.Input((npred, X.shape[1], X.shape[2]))\n",
    "    \n",
    "    if center_seq:\n",
    "        # mask [1,1,0] to ignore z-axis shift\n",
    "        doffsets = layers.Lambda(lambda x: K.constant([[[[1,1,0]]]])*K.mean(x,axis=(1,2),keepdims=True))(disc_input1)\n",
    "        H1 = layers.Subtract()([H1,doffsets])\n",
    "        H2 = layers.Subtract()([H2,doffsets])\n",
    "    \n",
    "    H1 = layers.Reshape((lookback, X.shape[1]*X.shape[2]))(H1)\n",
    "    H2 = layers.Reshape((npred, X.shape[1]*X.shape[2]))(H2)\n",
    "    \n",
    "    for nc in n_cells_disc[:-1]:\n",
    "        L = layers.CuDNNLSTM(nc, return_sequences=True)\n",
    "        H1 = L(H1)\n",
    "        H2 = L(H2)\n",
    "    \n",
    "    H = layers.Concatenate(axis=1)([H1,H2])\n",
    "    H = layers.CuDNNLSTM(n_cells_disc[-1])(H)\n",
    "    \n",
    "    H2 = layers.CuDNNLSTM(n_cells_disc[-1])(H2)\n",
    "    \n",
    "    H = layers.Concatenate()([H,H2])\n",
    "        \n",
    "    def s(l):\n",
    "        return layers.LeakyReLU()(l)\n",
    "    H = layers.Dense(128)(H)\n",
    "    H = s(H)\n",
    "    H = layers.Dense(1, activation='sigmoid', use_bias=False)(H)\n",
    "    disc_output = H\n",
    "    \n",
    "    discriminator = Model([disc_input1,disc_input2], disc_output)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    input_true = layers.Input((npred, X.shape[1], X.shape[2]))\n",
    "    \n",
    "    fg_output = generator([input_seq, input_noise])\n",
    "    full_output = discriminator([input_seq,fg_output])\n",
    "    full = Model([input_seq, input_noise], full_output)\n",
    "    \n",
    "    full.hp_rigidity_weight = K.variable(1.0)\n",
    "    full.hp_xe_weight = K.variable(1.0)\n",
    "    full.hp_mse_weight = K.variable(1.0)    \n",
    "\n",
    "    \n",
    "    def full_loss(y_true, y_pred):\n",
    "        y_ones = K.ones((K.shape(full_output)[0],1))\n",
    "        xe_loss = K.mean(K.binary_crossentropy(y_ones,full_output,), axis=-1)\n",
    "        \n",
    "        if use_mse:\n",
    "            mse_loss = K.mean(K.square(y_true - fg_output),axis=(-3,-2,-1))\n",
    "        \n",
    "        rigidity_loss = 0\n",
    "        for i,j in vtx_pairs[:n_rigid]:\n",
    "            a = fg_output[:,:,i]\n",
    "            b = fg_output[:,:,j]\n",
    "            d = vdist_mean[i,j]\n",
    "            v = vdist_var[i,j]\n",
    "            D = K.sum(K.square(a-b),axis=-1)\n",
    "            rigidity_loss += K.square(D-d)/v    \n",
    "        if n_rigid:\n",
    "            rigidity_loss /= n_rigid\n",
    "            rigidity_loss = K.mean(rigidity_loss, axis=-1)\n",
    "                \n",
    "        loss_total = full.hp_xe_weight*xe_loss\n",
    "        if n_rigid:\n",
    "            loss_total += full.hp_rigidity_weight*rigidity_loss\n",
    "        if use_mse:\n",
    "            loss_total += full.hp_mse_weight*mse_loss\n",
    "        return loss_total\n",
    "    \n",
    "    full.compile(loss=full_loss, optimizer='adam')\n",
    "    discriminator.trainable = True\n",
    "    \n",
    "    return generator, discriminator, full, probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 24\n",
    "npred = 1\n",
    "noise_dim = 32\n",
    "center_seq = True\n",
    "n_cells = (96,64,32)\n",
    "n_cells_disc = (48,48,48,)\n",
    "use_mse = False\n",
    "\n",
    "gen, disc, full, probe = mk_model(lookback=lookback, npred=npred,\n",
    "                                  n_cells=n_cells, n_cells_disc=n_cells_disc,\n",
    "                                  noise_dim=noise_dim, center_seq=center_seq,\n",
    "                                  n_rigid=n_rigid, use_mse=use_mse)\n",
    "gen.summary()\n",
    "disc.summary()\n",
    "disc.trainable = False\n",
    "full.summary()\n",
    "disc.trainable = True\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "rscores = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(full.optimizer.lr, 1e-4)\n",
    "K.set_value(disc.optimizer.lr, 1e-5)\n",
    "\n",
    "K.set_value(full.hp_rigidity_weight, 1e-4)\n",
    "K.set_value(full.hp_xe_weight, 1)\n",
    "K.set_value(full.hp_mse_weight, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 128\n",
    "do_rotation = True\n",
    "\n",
    "nbatches = (len(X)-batch_size*lookback-npred)//batch_size\n",
    "\n",
    "#y_real = to_categorical(np.ones(batch_size),2)\n",
    "#y_fake = to_categorical(np.zeros(batch_size),2)\n",
    "y_real = np.ones(batch_size)\n",
    "y_fake = np.zeros(batch_size)\n",
    "print(\"Nbatches =\", nbatches)\n",
    "for iepoch in range(epochs):\n",
    "    offsets = np.random.choice(len(X)-batch_size*lookback-npred, replace=False, size=(nbatches,batch_size))\n",
    "    \n",
    "    d_loss = 0\n",
    "    g_loss = 0\n",
    "    for ibatch in range(nbatches):\n",
    "        batch_idxs = offsets[ibatch].repeat(lookback).reshape(batch_size,lookback) + np.arange(lookback)\n",
    "        truth_idxs = offsets[ibatch].repeat(npred).reshape(batch_size,npred) + np.arange(npred) + lookback\n",
    "        \n",
    "        x1_real = X[batch_idxs]\n",
    "        x2_real = X[truth_idxs]\n",
    "        if do_rotation:\n",
    "            theta = 2*np.pi*np.random.rand()\n",
    "            c,s = np.cos(theta), np.sin(theta)\n",
    "            rot = np.array([[c,-s,0],[s,c,0],[0,0,1]])\n",
    "            x1_real = rot.dot(x1_real.transpose((0,1,3,2))).transpose((1,2,3,0))\n",
    "            x2_real = rot.dot(x2_real.transpose((0,1,3,2))).transpose((1,2,3,0))\n",
    "        \n",
    "        \n",
    "        noise = np.random.normal(size=(batch_size,noise_dim))\n",
    "        x2_fake = gen.predict([x1_real, noise])\n",
    "        \n",
    "        disc.trainable = True\n",
    "        l, a = disc.train_on_batch([x1_real, x2_real], y_real)\n",
    "        d_loss += 0.5*l\n",
    "        l,a = disc.train_on_batch([x1_real, x2_fake], y_fake)\n",
    "        d_loss += 0.5*l\n",
    "        \n",
    "        # TODO: figure out if it makes sense to redo translations/rotations before\n",
    "        # the generator training step.\n",
    "        \n",
    "        if do_rotation:\n",
    "            theta = 2*np.pi*np.random.rand()\n",
    "            c,s = np.cos(theta), np.sin(theta)\n",
    "            rot = np.array([[c,-s,0],[s,c,0],[0,0,1]])\n",
    "            x1_real = rot.dot(x1_real.transpose((0,1,3,2))).transpose((1,2,3,0))\n",
    "            x2_real = rot.dot(x2_real.transpose((0,1,3,2))).transpose((1,2,3,0))\n",
    "        \n",
    "        disc.trainable = False\n",
    "        noise = np.random.normal(size=(batch_size,noise_dim))\n",
    "        g_loss += full.train_on_batch([x1_real, noise], x2_real)\n",
    "        \n",
    "    d_losses.append(d_loss/nbatches)\n",
    "    g_losses.append(g_loss/nbatches)\n",
    "    print(\"Epoch %d/%d: L(d)=%.2e L(g)=%.2e\" % (iepoch, epochs, d_losses[-1], g_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d_losses[:], label='disc')\n",
    "plt.plot(g_losses[:], label='gen')\n",
    "plt.legend();\n",
    "#plt.xlim(left=250)\n",
    "#plt.ylim(top=0.75, bottom=.65)\n",
    "#plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen.load_weights('models/gan-gen-e1656.wt.h5')\n",
    "#disc.load_weights('models/gan-disc-e1656.wt.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iprompt = np.random.randint(len(X)-batch_size*lookback-1)\n",
    "ngen = 9*lookback\n",
    "\n",
    "gen_seq = np.zeros((lookback+ngen, X.shape[1], X.shape[2]))\n",
    "gen_seq[:lookback] = X[iprompt:iprompt+lookback]\n",
    "\n",
    "for i in range(ngen):\n",
    "    next_frame = gen.predict([np.expand_dims(gen_seq[i:i+lookback],0),np.random.normal(size=(1,noise_dim))])[0,0]\n",
    "    gen_seq[i+lookback] = next_frame\n",
    "\n",
    "# plot the first generated frame\n",
    "#plot_pose(gen_seq[lookback])\n",
    "ax = plot_stick(gen_seq[lookback], vtx_pairs[:n_rigid], figsize=(8,8))\n",
    "plot_pose(gen_seq[lookback], ax)\n",
    "plt.title(\"Epoch %d\"%(len(g_losses)-1), fontsize=16)\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('real')\n",
    "plt.hist([gen_seq[:lookback,:,0].flatten(),\n",
    "          gen_seq[:lookback,:,1].flatten(),\n",
    "          gen_seq[:lookback,:,2].flatten()], histtype='step', bins=20, range=(-1.5,1.5));\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('gen')\n",
    "plt.hist([gen_seq[lookback:,:,0].flatten(),\n",
    "          gen_seq[lookback:,:,1].flatten(),\n",
    "          gen_seq[lookback:,:,2].flatten()], histtype='step', bins=20, range=(-1.5,1.5));\n",
    "\n",
    "dm = np.zeros_like(vdist_mean)\n",
    "dv = np.zeros_like(vdist_var)\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(i+1,X.shape[1]):\n",
    "        dm[i,j] = dm[j,i] = np.sqrt((gen_seq[lookback:,i]-gen_seq[lookback:,j])**2).sum(axis=-1).mean()\n",
    "        dv[i,j] = dv[j,i] = np.sqrt((gen_seq[lookback:,i]-gen_seq[lookback:,j])**2).sum(axis=-1).var(ddof=1)\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(vdist_mean, vmin=0, vmax=vdist_mean.max())\n",
    "plt.title('real')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('gen')\n",
    "plt.imshow(dm, vmin=0, vmax=vdist_mean.max())\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.sqrt(vdist_var), vmin=0, vmax=np.sqrt(vdist_var).max())\n",
    "plt.title('real')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('gen')\n",
    "plt.imshow(np.sqrt(dv), vmin=0, vmax=np.sqrt(vdist_var).max());\n",
    "\n",
    "rigidity_score = 0\n",
    "for i,j in vtx_pairs[:n_rigid]:\n",
    "    rigidity_score += ((np.sum((x2_fake[:,:,i]-x2_fake[:,:,j])**2,axis=-1)-vdist_mean[i,j])**2)/vdist_var[i,j]\n",
    "if n_rigid:\n",
    "    rigidity_score /= n_rigid\n",
    "rigidity_score = rigidity_score.mean()\n",
    "\n",
    "print(\"Rigidity score: %.2e\"%rigidity_score)\n",
    "rscores[len(g_losses)] = rigidity_score\n",
    "\n",
    "plt.figure();\n",
    "rs = np.array(sorted(rscores.items()))\n",
    "plt.plot(rs[:,0], rs[:,1])\n",
    "plt.xlim(left=0)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(animate(gen_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
